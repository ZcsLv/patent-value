{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n",
      "57\n",
      "9985\n"
     ]
    }
   ],
   "source": [
    "#打开excel\n",
    "wb = xlrd.open_workbook('../data/1.xls')\n",
    "#按工作簿定位工作表\n",
    "sh = wb.sheet_by_name('下载的著录项1')\n",
    "print(sh.nrows)#有效数据行数\n",
    "print(sh.ncols)#有效数据列数\n",
    "# print(sh.cell(0,0).value)#输出第一行第一列的值\n",
    "# print(sh.row_values(0))#输出第一行的所有值\n",
    "#将数据和标题组合成字典\n",
    "# print(dict(zip(sh.row_values(0),sh.row_values(1))))\n",
    "\n",
    "datas={}\n",
    "train_datasets=[]\n",
    "grades=[]\n",
    "#遍历excel，打印所有数据\n",
    "for i in range(1,sh.nrows):\n",
    "    datas={}\n",
    "    # 如果独立要求数量为0 则跳过这条数据\n",
    "    if sh.row_values(i)[10]=='':\n",
    "        continue\n",
    "    if type(sh.row_values(i)[10])=='str':\n",
    "        sh.row_values(i)[10]=int(sh.row_values(i)[10])\n",
    "    if type(sh.row_values(i)[11])=='str':\n",
    "        sh.row_values(i)[11]=int(sh.row_values(i)[11])\n",
    "    if type(sh.row_values(i)[13])=='str':\n",
    "        sh.row_values(i)[13]=int(sh.row_values(i)[13])\n",
    "    datas['title']=sh.row_values(i)[2]\n",
    "    datas['abstract']=sh.row_values(i)[3]\n",
    "    datas['ind_claims']=sh.row_values(i)[10]\n",
    "    datas['dep_claims']=sh.row_values(i)[11]\n",
    "    datas['len_claims']=sh.row_values(i)[13]\n",
    "    datas['len_abstract']=len(sh.row_values(i)[3])\n",
    "    datas['back_incitions']=sh.row_values(i)[33]\n",
    "    datas['forward_incitions']=sh.row_values(i)[34]\n",
    "    datas['num_inventors']=int(sh.row_values(i)[28])\n",
    "    datas['num_family']=int(sh.row_values(i)[40])\n",
    "    datas['cpcs']=sh.row_values(i)[20].count(';')+1\n",
    "    datas['ipcs']=sh.row_values(i)[21].count(';')+1\n",
    "    train_datasets.append(datas)\n",
    "print(len(train_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7336\n",
      "57\n",
      "2999\n"
     ]
    }
   ],
   "source": [
    "#打开test excel\n",
    "wb = xlrd.open_workbook('../data/7.xls')\n",
    "#按工作簿定位工作表\n",
    "sh = wb.sheet_by_name('下载的著录项1')\n",
    "print(sh.nrows)#有效数据行数\n",
    "print(sh.ncols)#有效数据列数\n",
    "# print(sh.cell(0,0).value)#输出第一行第一列的值\n",
    "# print(sh.row_values(0))#输出第一行的所有值\n",
    "# 将数据和标题组合成字典\n",
    "# print(dict(zip(sh.row_values(0),sh.row_values(1))))\n",
    "test_datasets=[]\n",
    "test_grades=[]\n",
    "# 遍历excel，打印所有数据\n",
    "# 取前面3000个数据作为测试数据\n",
    "for i in range(1,3001):\n",
    "    datas={}\n",
    "    # 如果独立要求数量为0 则跳过这条数据\n",
    "    if sh.row_values(i)[10]=='':\n",
    "        continue\n",
    "    if type(sh.row_values(i)[10])=='str':\n",
    "        sh.row_values(i)[10]=int(sh.row_values(i)[10])\n",
    "    if type(sh.row_values(i)[11])=='str':\n",
    "        sh.row_values(i)[11]=int(sh.row_values(i)[11])\n",
    "    if type(sh.row_values(i)[13])=='str':\n",
    "        sh.row_values(i)[13]=int(sh.row_values(i)[13])\n",
    "    datas['title']=sh.row_values(i)[2]\n",
    "    datas['abstract']=sh.row_values(i)[3]\n",
    "    datas['ind_claims']=sh.row_values(i)[10]\n",
    "    datas['dep_claims']=sh.row_values(i)[11]\n",
    "    datas['len_claims']=sh.row_values(i)[13]\n",
    "    datas['len_abstract']=len(sh.row_values(i)[3])\n",
    "    datas['back_incitions']=sh.row_values(i)[33]\n",
    "    datas['forward_incitions']=sh.row_values(i)[34]\n",
    "    datas['num_inventors']=int(sh.row_values(i)[28])\n",
    "    datas['num_family']=int(sh.row_values(i)[40])\n",
    "    datas['cpcs']=sh.row_values(i)[20].count(';')+1\n",
    "    datas['ipcs']=sh.row_values(i)[21].count(';')+1\n",
    "    test_datasets.append(datas)\n",
    "print(len(test_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "#打开test excel\n",
    "wb = xlrd.open_workbook('../data/2.xls')\n",
    "#按工作簿定位工作表\n",
    "sh = wb.sheet_by_name('下载的著录项1')\n",
    "print(sh.nrows)#有效数据行数\n",
    "print(sh.ncols)#有效数据列数\n",
    "# print(sh.cell(0,0).value)#输出第一行第一列的值\n",
    "# print(sh.row_values(0))#输出第一行的所有值\n",
    "# 将数据和标题组合成字典\n",
    "# print(dict(zip(sh.row_values(0),sh.row_values(1))))\n",
    "val_datasets=[]\n",
    "# 遍历excel，打印所有数据\n",
    "# 取前面3000个数据作为测试数据\n",
    "for i in range(1,4001):\n",
    "    datas={}\n",
    "    # 如果独立要求数量为0 则跳过这条数据\n",
    "    if sh.row_values(i)[10]=='':\n",
    "        continue\n",
    "    if type(sh.row_values(i)[10])=='str':\n",
    "        sh.row_values(i)[10]=int(sh.row_values(i)[10])\n",
    "    if type(sh.row_values(i)[11])=='str':\n",
    "        sh.row_values(i)[11]=int(sh.row_values(i)[11])\n",
    "    if type(sh.row_values(i)[13])=='str':\n",
    "        sh.row_values(i)[13]=int(sh.row_values(i)[13])\n",
    "    datas['title']=sh.row_values(i)[2]\n",
    "    datas['abstract']=sh.row_values(i)[3]\n",
    "    datas['ind_claims']=sh.row_values(i)[10]\n",
    "    datas['dep_claims']=sh.row_values(i)[11]\n",
    "    datas['len_claims']=sh.row_values(i)[13]\n",
    "    datas['len_abstract']=len(sh.row_values(i)[3])\n",
    "    datas['back_incitions']=sh.row_values(i)[33]\n",
    "    datas['forward_incitions']=sh.row_values(i)[34]\n",
    "    datas['num_inventors']=int(sh.row_values(i)[28])\n",
    "    datas['num_family']=int(sh.row_values(i)[40])\n",
    "    datas['cpcs']=sh.row_values(i)[20].count(';')+1\n",
    "    datas['ipcs']=sh.row_values(i)[21].count(';')+1\n",
    "    val_datasets.append(datas)\n",
    "print(len(val_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 forward_incitions 为空的情况\n",
    "def pro_null(train_datasets):\n",
    "    for i in range(len(train_datasets)):\n",
    "        if train_datasets[i]['forward_incitions']=='':\n",
    "            train_datasets[i]['forward_incitions']=0\n",
    "        else:\n",
    "            train_datasets[i]['forward_incitions']=int(train_datasets[i]['forward_incitions'])\n",
    "    # 处理 back_incitions 为空的情况\n",
    "    for i in range(len(train_datasets)):\n",
    "        if train_datasets[i]['back_incitions']=='':\n",
    "            train_datasets[i]['back_incitions']=0\n",
    "        else:\n",
    "            train_datasets[i]['back_incitions']=int(train_datasets[i]['back_incitions'])\n",
    "    # 处理 dep_claims 为空的情况\n",
    "    for i in range(len(train_datasets)):\n",
    "        if train_datasets[i]['dep_claims']=='':\n",
    "            train_datasets[i]['dep_claims']=0\n",
    "    # 处理 len_claims 为空的情况\n",
    "    for i in range(len(train_datasets)):\n",
    "        if train_datasets[i]['len_claims']=='':\n",
    "            train_datasets[i]['len_claims']=0\n",
    "    # 处理 num_inventors 为空的情况\n",
    "    for i in range(len(train_datasets)):\n",
    "        if train_datasets[i]['num_inventors']=='':\n",
    "            train_datasets[i]['num_inventors']=0\n",
    "    # 处理 num_family 为空的情况\n",
    "    for i in range(len(train_datasets)):\n",
    "        if train_datasets[i]['num_family']=='':\n",
    "            train_datasets[i]['num_family']=0\n",
    "    # 处理 cpcs 为空的情况\n",
    "    for i in range(len(train_datasets)):\n",
    "        if train_datasets[i]['cpcs']=='':\n",
    "            train_datasets[i]['cpcs']=0\n",
    "    # 处理 ipcs 为空的情况\n",
    "    for i in range(len(train_datasets)):\n",
    "        if train_datasets[i]['ipcs']=='':\n",
    "            train_datasets[i]['ipcs']=0\n",
    "    return train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把几个属性 从str转为int\n",
    "def str2int(train_datasets):\n",
    "    for i in range(len(train_datasets)):\n",
    "        if type(train_datasets[i]['ind_claims'])=='str':\n",
    "            train_datasets[i]['ind_claims']=int(train_datasets[i]['ind_claims'])\n",
    "        if type(train_datasets[i]['dep_claims'])=='str':\n",
    "            train_datasets[i]['dep_claims']=int(train_datasets[i]['dep_claims'])\n",
    "        if type(train_datasets[i]['len_claims'])=='str':\n",
    "            train_datasets[i]['len_claims']=int(train_datasets[i]['len_claims'])\n",
    "    return train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datasets=pro_null(val_datasets)\n",
    "val_datasets=str2int(val_datasets)\n",
    "val_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 forward_incitions 为空的情况\n",
    "for i in range(len(train_datasets)):\n",
    "    if train_datasets[i]['forward_incitions']=='':\n",
    "        train_datasets[i]['forward_incitions']=0\n",
    "    else:\n",
    "        train_datasets[i]['forward_incitions']=int(train_datasets[i]['forward_incitions'])\n",
    "for i in range(len(test_datasets)):\n",
    "    if test_datasets[i]['forward_incitions']=='':\n",
    "        test_datasets[i]['forward_incitions']=0\n",
    "    else:\n",
    "        test_datasets[i]['forward_incitions']=int(test_datasets[i]['forward_incitions'])\n",
    "# 处理 back_incitions 为空的情况\n",
    "for i in range(len(train_datasets)):\n",
    "    if train_datasets[i]['back_incitions']=='':\n",
    "        train_datasets[i]['back_incitions']=0\n",
    "    else:\n",
    "        train_datasets[i]['back_incitions']=int(train_datasets[i]['back_incitions'])\n",
    "for i in range(len(test_datasets)):\n",
    "    if test_datasets[i]['back_incitions']=='':\n",
    "        test_datasets[i]['back_incitions']=0\n",
    "    else:\n",
    "        test_datasets[i]['back_incitions']=int(test_datasets[i]['back_incitions'])\n",
    "# 处理 dep_claims 为空的情况\n",
    "for i in range(len(train_datasets)):\n",
    "    if train_datasets[i]['dep_claims']=='':\n",
    "        train_datasets[i]['dep_claims']=0\n",
    "for i in range(len(test_datasets)):\n",
    "    if test_datasets[i]['dep_claims']=='':\n",
    "        test_datasets[i]['dep_claims']=0\n",
    "# 处理 len_claims 为空的情况\n",
    "for i in range(len(train_datasets)):\n",
    "    if train_datasets[i]['len_claims']=='':\n",
    "        train_datasets[i]['len_claims']=0\n",
    "for i in range(len(test_datasets)):\n",
    "    if test_datasets[i]['len_claims']=='':\n",
    "        test_datasets[i]['len_claims']=0\n",
    "# 处理 num_inventors 为空的情况\n",
    "for i in range(len(train_datasets)):\n",
    "    if train_datasets[i]['num_inventors']=='':\n",
    "        train_datasets[i]['num_inventors']=0\n",
    "for i in range(len(test_datasets)):\n",
    "    if test_datasets[i]['num_inventors']=='':\n",
    "        test_datasets[i]['num_inventors']=0\n",
    "# 处理 num_family 为空的情况\n",
    "for i in range(len(train_datasets)):\n",
    "    if train_datasets[i]['num_family']=='':\n",
    "        train_datasets[i]['num_family']=0\n",
    "for i in range(len(test_datasets)):\n",
    "    if test_datasets[i]['num_family']=='':\n",
    "        test_datasets[i]['num_family']=0\n",
    "# 处理 cpcs 为空的情况\n",
    "for i in range(len(train_datasets)):\n",
    "    if train_datasets[i]['cpcs']=='':\n",
    "        train_datasets[i]['cpcs']=0\n",
    "for i in range(len(test_datasets)):\n",
    "    if test_datasets[i]['cpcs']=='':\n",
    "        test_datasets[i]['ipcs']=0\n",
    "# 处理 ipcs 为空的情况\n",
    "for i in range(len(train_datasets)):\n",
    "    if train_datasets[i]['ipcs']=='':\n",
    "        train_datasets[i]['ipcs']=0\n",
    "for i in range(len(test_datasets)):\n",
    "    if test_datasets[i]['ipcs']=='':\n",
    "        test_datasets[i]['ipcs']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把几个属性 从str转为int\n",
    "for i in range(len(train_datasets)):\n",
    "    if type(train_datasets[i]['ind_claims'])=='str':\n",
    "        train_datasets[i]['ind_claims']=int(train_datasets[i]['ind_claims'])\n",
    "    if type(train_datasets[i]['dep_claims'])=='str':\n",
    "        train_datasets[i]['dep_claims']=int(train_datasets[i]['dep_claims'])\n",
    "    if type(train_datasets[i]['len_claims'])=='str':\n",
    "        train_datasets[i]['len_claims']=int(train_datasets[i]['len_claims'])\n",
    "for i in range(len(test_datasets)):\n",
    "    if type(test_datasets[i]['ind_claims'])=='str':\n",
    "        test_datasets[i]['ind_claims']=int(test_datasets[i]['ind_claims'])\n",
    "    if type(test_datasets[i]['dep_claims'])=='str':\n",
    "        test_datasets[i]['dep_claims']=int(test_datasets[i]['dep_claims'])\n",
    "    if type(test_datasets[i]['len_claims'])=='str':\n",
    "        test_datasets[i]['len_claims']=int(test_datasets[i]['len_claims'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40744\\1321511388.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mk2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrade\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_datasets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_datasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40744\\1321511388.py\u001b[0m in \u001b[0;36mgrade\u001b[1;34m(train_datasets, test_datasets)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mn2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;34m' 确定第n大的数'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0marr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'forward_incitions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_datasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0marr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mk1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40744\\1321511388.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mn2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;34m' 确定第n大的数'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0marr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'forward_incitions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_datasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0marr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mk1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "\n",
    "# 根据 forward_incitions 次数确定grade \n",
    "def grade(train_datasets,test_datasets):\n",
    "    ' 确定总个数m'\n",
    "    m=len(train_datasets)+len(test_datasets)\n",
    "    ' 确定前20%的个数n'\n",
    "    n1=int(m*0.2)\n",
    "    ' 确定前20-60%的个数n'\n",
    "    n2=int(m*0.6)-int(m*0.2)\n",
    "    ' 确定前60-100%的个数n'\n",
    "    n2=m-n1-n2\n",
    "    ' 确定第n大的数'\n",
    "    arr=[int(train_datasets[i]['forward_incitions']) for i in range(len(train_datasets))]\n",
    "    arr=sorted(arr,reverse=True)\n",
    "    k1=arr[n1-1]\n",
    "    k2=arr[n2-1]  \n",
    "    return k1,k2\n",
    "k1,k2=grade(train_datasets,test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 补充grade 属性\n",
    "for i in range(len(train_datasets)):\n",
    "    if train_datasets[i]['forward_incitions']>=k1:\n",
    "        train_datasets[i]['grade']=1\n",
    "    elif train_datasets[i]['forward_incitions']>=k2 and train_datasets[i]['forward_incitions']<k1 :\n",
    "        train_datasets[i]['grade']=2\n",
    "    else:\n",
    "        train_datasets[i]['grade']=3\n",
    "for i in range(len(test_datasets)):\n",
    "    if test_datasets[i]['forward_incitions']>=k1:\n",
    "        test_datasets[i]['grade']=1\n",
    "    elif test_datasets[i]['forward_incitions']>=k2 and test_datasets[i]['forward_incitions']<k1 :\n",
    "        test_datasets[i]['grade']=2\n",
    "    else:\n",
    "        test_datasets[i]['grade']=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存到pickle文件中，方便下次读取\n",
    "import pickle\n",
    "with open(\"../data/indictors11_train.pkl\",\"wb\") as f:\n",
    "    pickle.dump(train_datasets, f)\n",
    "with open(\"../data/indictors11_test.pkl\",\"wb\") as f:\n",
    "    pickle.dump(test_datasets, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK, PAD = '<UNK>', '<PAD>'\n",
    "import pickle\n",
    "def load_dataset(file_path,pad_size=32):\n",
    "    contents = []\n",
    "    # 分词处理的匿名函数\n",
    "    tokenizer = lambda x: [y for y in x]  # char-level\n",
    "    with open(file_path,\"rb\") as f:\n",
    "        datas=pickle.load(f)\n",
    "        for i in range(len(datas)):\n",
    "            lin = datas[i]\n",
    "            content,label = lin['abstract'],lin['grade']\n",
    "            # indictors=[lin['ind_claims'],lin['dep_claims'],lin['len_abstract'],\n",
    "            # lin['len_claims'],lin['num_inventors'],lin['back_incitions'],lin['num_family'],lin['cpcs'],lin['ipcs']]\n",
    "            indictors=[int(lin['ind_claims']),int(lin['dep_claims']),int(lin['len_abstract']),\n",
    "            int(lin['len_claims']),int(lin['num_inventors']),int(lin['back_incitions']),int(lin['num_family']),int(lin['cpcs']),int(lin['ipcs'])]\n",
    "            words_line = []\n",
    "            token = tokenizer(content)\n",
    "            seq_len = len(token)\n",
    "            if pad_size:\n",
    "                if len(token) < pad_size:\n",
    "                    token.extend([PAD] * (pad_size - len(token)))\n",
    "                else:\n",
    "                    token = token[:pad_size]\n",
    "                    seq_len = pad_size\n",
    "            # word to id\n",
    "            #  从pkl文件中读出来保存好了的词表\n",
    "            # vocab=pickle.load(open(vocab_path,'rb'))\n",
    "            # for word in token:\n",
    "            #     words_line.append(vocab.get(word, vocab.get(UNK)))\n",
    "            contents.append(( content,indictors,int(label), seq_len))\n",
    "    return contents  # [([...], [],0,句子长度), ([...], 1), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开文件\n",
    "data=load_dataset(\"../data/indictors11_train.pkl\",pad_size=32)\n",
    "print(len(data))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3011\n"
     ]
    }
   ],
   "source": [
    "path=\"../data/indictors11_test.pkl\"\n",
    "import pickle\n",
    "with open(path,\"rb\") as f:\n",
    "    datas=pickle.load(f)\n",
    "datas\n",
    "print(len(datas))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4a847150c17a56e8ab8e8e7872ca30cb83471eb1df49153297cb018a7672186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
